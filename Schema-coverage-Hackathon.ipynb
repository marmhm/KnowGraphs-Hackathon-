{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fa619d-8402-4815-b6df-cfd313952c2a",
   "metadata": {},
   "source": [
    "# Code for Hackathon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Method:\n",
    "\n",
    "- get unique queries (does not handle the white spaces)\n",
    "- parse queries\n",
    "- get normalized queries\n",
    "- get unique queries from normalized verion and parsed tree(to handle white space problem)\n",
    "- profile table: Keyword count in queries\n",
    "(SELECT, Ask, Describe, Construct, Distinct, Limit, Offset, Order By, Filter, And, Union, Opt, Graph, Not Exists, Minus, Exists, Count, Max, Min, Avg, Sum, Group By, Having)\n",
    "- get distributaion of triple patterns\n",
    "- cluster queries based on their similarity\n",
    "- schema coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We download the query logs from \"link to download the logs\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data//bio2rdf_sparql_logs_processed_01-2019_to_07-2021.csv', lineterminator='\\n', dtype=str, encoding='utf-8')\n",
    "\n",
    "# Count the number of null values in each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "print(\"Null value counts:\")\n",
    "print(null_counts)\n",
    "print(len(df))\n",
    "pd.set_option('display.max_colwidth', None)  # Display full content of columns\n",
    "print(df.head())\n",
    "## Number of queries are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. We extract unique queries and their count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this code dose not handle whitespaces to identify unique queries. after parsing queries I use parse tree to identify unique queries. current step is only for optimizing query parsing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique queries extracted with counts and written to new_unique_bio2rdf_sparql_logs_with_counts.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def extract_unique_queries(input_file, output_file):\n",
    "    try:\n",
    "        # Initialize a dictionary to store query counts\n",
    "        query_counts = {}\n",
    "\n",
    "        with open(input_file, 'r', newline='\\n', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "\n",
    "            # Iterate through the rows and count queries\n",
    "            for row in reader:\n",
    "                query = row['query']  # Assuming 'query' is the correct column name\n",
    "                query_counts[query] = query_counts.get(query, 0) + 1\n",
    "\n",
    "        # Sort queries and their counts based on the count column in descending order\n",
    "        sorted_queries = sorted(query_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        # Write sorted queries and their counts to the output CSV file\n",
    "        with open(output_file, 'w', newline='\\n', encoding='utf-8') as outfile:\n",
    "            writer = csv.writer(outfile, lineterminator='\\n')\n",
    "            writer.writerow(['query', 'count'])  # Write header\n",
    "\n",
    "            for query, count in sorted_queries:\n",
    "                writer.writerow([query, count])\n",
    "\n",
    "        print(\"Unique queries extracted with counts and written to\", output_file)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input CSV file not found.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Replace 'input_file.csv' with the name of your CSV file and 'output_file.csv' with the desired output filename\n",
    "extract_unique_queries('new_removedformats_bio2rdf_logs.csv', 'new_unique_bio2rdf_sparql_logs_with_counts.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1051679\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASKWHERE  { ?s ?p ?o}</td>\n",
       "      <td>539922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT  ?sWHERE  { ?s ?p ?o}LIMIT   1</td>\n",
       "      <td>365078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT  ?tWHERE  { ?x  ?y  ?t }LIMIT   1</td>\n",
       "      <td>244841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASKWHERE  { ?s  ?p  ?o }</td>\n",
       "      <td>83599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT  (COUNT(?c) AS ?x)WHERE  { ?s &lt;http://w...</td>\n",
       "      <td>44671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SELECT  (COUNT(?p) AS ?x)WHERE  { ?s ?p ?o}</td>\n",
       "      <td>44662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SELECT ?s WHERE  { ?s ?p ?o } LIMIT 1</td>\n",
       "      <td>35443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASK WHERE { ?s ?p ?o }</td>\n",
       "      <td>35441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SELECT  *WHERE  { ?s ?p ?o}LIMIT   1</td>\n",
       "      <td>13762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SELECT  *WHERE  { ?s ?p ?o}LIMIT   100002</td>\n",
       "      <td>13701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SELECT  *WHERE  { ?s ?p ?o}LIMIT   12500</td>\n",
       "      <td>13615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SELECT  *WHERE  { ?s ?p ?o}LIMIT   25001</td>\n",
       "      <td>13610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SELECT  *WHERE  { ?s ?p ?o}LIMIT   3125</td>\n",
       "      <td>13606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SELECT  *WHERE  { ?s ?p ?o}LIMIT   5</td>\n",
       "      <td>13601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SELECT  *WHERE  { ?s ?p ?o}LIMIT   50001</td>\n",
       "      <td>13595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SELECT  *WHERE  { ?s ?p ?o}LIMIT   6250</td>\n",
       "      <td>13586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ASK {?s ?p ?o}</td>\n",
       "      <td>4732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SELECT * WHERE { ?s ?p ?o } LIMIT 10</td>\n",
       "      <td>3164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SELECT  ?tWHERE  { ?x  a                     ?...</td>\n",
       "      <td>3151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ASKWHERE  {  }</td>\n",
       "      <td>2668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CONSTRUCT   { ?x &lt;http://example.org/type&gt; ?v ...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SELECT  ?yWHERE  { GRAPH ?g      { { SELECT  *...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SELECT  ?yWHERE  { { SELECT  *      WHERE     ...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SELECT  *WHERE  { &lt;http://nonsensical.com/1&gt; &lt;...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SELECT  *WHERE  { &lt;http://nonsensical.com/1&gt; &lt;...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SELECT  ?oWHERE  { &lt;http://nonsensical.com/1&gt; ...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SELECT  *WHERE  { &lt;http://nonsensical.com/1&gt; &lt;...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SELECT  *WHERE  { ?s &lt;http://example.org/type&gt;...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SELECT  ?s ?strWHERE  { &lt;http://nonsensical.co...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SELECT  *WHERE  { &lt;http://nonsensical.com/1&gt; &lt;...</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query   count\n",
       "0                               ASKWHERE  { ?s ?p ?o}  539922\n",
       "1               SELECT  ?sWHERE  { ?s ?p ?o}LIMIT   1  365078\n",
       "2            SELECT  ?tWHERE  { ?x  ?y  ?t }LIMIT   1  244841\n",
       "3                            ASKWHERE  { ?s  ?p  ?o }   83599\n",
       "4   SELECT  (COUNT(?c) AS ?x)WHERE  { ?s <http://w...   44671\n",
       "5         SELECT  (COUNT(?p) AS ?x)WHERE  { ?s ?p ?o}   44662\n",
       "6               SELECT ?s WHERE  { ?s ?p ?o } LIMIT 1   35443\n",
       "7                              ASK WHERE { ?s ?p ?o }   35441\n",
       "8                SELECT  *WHERE  { ?s ?p ?o}LIMIT   1   13762\n",
       "9           SELECT  *WHERE  { ?s ?p ?o}LIMIT   100002   13701\n",
       "10           SELECT  *WHERE  { ?s ?p ?o}LIMIT   12500   13615\n",
       "11           SELECT  *WHERE  { ?s ?p ?o}LIMIT   25001   13610\n",
       "12            SELECT  *WHERE  { ?s ?p ?o}LIMIT   3125   13606\n",
       "13               SELECT  *WHERE  { ?s ?p ?o}LIMIT   5   13601\n",
       "14           SELECT  *WHERE  { ?s ?p ?o}LIMIT   50001   13595\n",
       "15            SELECT  *WHERE  { ?s ?p ?o}LIMIT   6250   13586\n",
       "16                                     ASK {?s ?p ?o}    4732\n",
       "17               SELECT * WHERE { ?s ?p ?o } LIMIT 10    3164\n",
       "18  SELECT  ?tWHERE  { ?x  a                     ?...    3151\n",
       "19                                     ASKWHERE  {  }    2668\n",
       "20  CONSTRUCT   { ?x <http://example.org/type> ?v ...    2648\n",
       "21  SELECT  ?yWHERE  { GRAPH ?g      { { SELECT  *...    2648\n",
       "22  SELECT  ?yWHERE  { { SELECT  *      WHERE     ...    2648\n",
       "23  SELECT  *WHERE  { <http://nonsensical.com/1> <...    2648\n",
       "24  SELECT  *WHERE  { <http://nonsensical.com/1> <...    2648\n",
       "25  SELECT  ?oWHERE  { <http://nonsensical.com/1> ...    2648\n",
       "26  SELECT  *WHERE  { <http://nonsensical.com/1> <...    2648\n",
       "27  SELECT  *WHERE  { ?s <http://example.org/type>...    2648\n",
       "28  SELECT  ?s ?strWHERE  { <http://nonsensical.co...    2648\n",
       "29  SELECT  *WHERE  { <http://nonsensical.com/1> <...    2648"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('new_unique_bio2rdf_sparql_logs_with_counts.csv', lineterminator='\\n', dtype=str)\n",
    "\n",
    "print(len(df))\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parse the queries:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- download Node.js from the official website: https://nodejs.org/\n",
    "\n",
    "- Installed Node js \n",
    "\n",
    "- npm install sparqljs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!npm install csv-parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install csv-stringify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a python file with following content(mine is called \"optimized_from_js_.py\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def run_script():\n",
    "    # Run the JavaScript code using Node.js as a subprocess\n",
    "    result = subprocess.run(['node', 'optimized.js'], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output (JSON representation of the parsed SPARQL query)\n",
    "    print(result.stdout)\n",
    "    print('hi')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  In Unix-like operating systems\n",
    "!nohup python optimized_from_js_.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In windows\n",
    "!pythonw optimized_from_js_.py > output90.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1051679\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>parsed_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"ASK\",\"where\":[{\"type\":\"bgp\",\"tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"ASK\",\"where\":[{\"type\":\"bgp\",\"tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"expressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"expressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"ASK\",\"where\":[{\"type\":\"bgp\",\"tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{}],\"where\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{}],\"where\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "1  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "2  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "3  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "4  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "5  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "6  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "7  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "8  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "9  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   \n",
       "\n",
       "                                        parsed_query  \n",
       "0  {\"queryType\":\"ASK\",\"where\":[{\"type\":\"bgp\",\"tri...  \n",
       "1  {\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...  \n",
       "2  {\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...  \n",
       "3  {\"queryType\":\"ASK\",\"where\":[{\"type\":\"bgp\",\"tri...  \n",
       "4  {\"queryType\":\"SELECT\",\"variables\":[{\"expressio...  \n",
       "5  {\"queryType\":\"SELECT\",\"variables\":[{\"expressio...  \n",
       "6  {\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...  \n",
       "7  {\"queryType\":\"ASK\",\"where\":[{\"type\":\"bgp\",\"tri...  \n",
       "8  {\"queryType\":\"SELECT\",\"variables\":[{}],\"where\"...  \n",
       "9  {\"queryType\":\"SELECT\",\"variables\":[{}],\"where\"...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = ['query', 'parsed_query']\n",
    "\n",
    "df = pd.read_csv('new_parsed.csv', lineterminator='\\n', dtype=str, header=None, names=column_names)\n",
    "\n",
    "print(len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following code returns valid queries and their parsed tree(removes unvalid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_file_path = 'new_parsed.csv'\n",
    "output_file_path = 'valid_parsed.csv'\n",
    "\n",
    "# Open the input CSV file for reading\n",
    "with open(input_file_path, 'r', newline='\\n', encoding='utf-8') as input_file:\n",
    "    # Create a CSV reader without header\n",
    "    reader = csv.reader(input_file)\n",
    "    \n",
    "    # Create a list to store valid rows\n",
    "    valid_rows = []\n",
    "    \n",
    "    for row in reader:\n",
    "        # Check if the second column is not equal to 'Error parsing the query'\n",
    "        if len(row) >= 2 and row[1] != 'Error parsing the query.':\n",
    "            valid_rows.append(row)\n",
    "\n",
    "# Open the output CSV file for writing\n",
    "with open(output_file_path, 'w', newline='\\n', encoding='utf-8') as output_file:\n",
    "    # Create a CSV writer with column names\n",
    "    writer = csv.writer(output_file)\n",
    "    \n",
    "    # Write column names\n",
    "    writer.writerow(['query', 'parsed_query'])\n",
    "    \n",
    "    # Write the valid rows\n",
    "    writer.writerows(valid_rows)\n",
    "\n",
    "print(f\"Valid rows have been written to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# column_names = ['query', 'parsed_query']\n",
    "\n",
    "df = pd.read_csv('valid_parsed.csv', lineterminator='\\n', dtype=str)\n",
    "\n",
    "print(len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in the 'parsed_query' column: 633453\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('valid_parsed.csv', lineterminator='\\n', dtype=str)\n",
    "\n",
    "# Count unique values in the \"ColumnName\" column\n",
    "unique_count = df['parsed_query\\r'].nunique()\n",
    "\n",
    "print(f\"Number of unique values in the 'parsed_query' column: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280e198-d380-44d0-8985-454dc10d7fb3",
   "metadata": {},
   "source": [
    "# Global variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6c06f-e5b3-4d32-9fd7-4ed7958661f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def normalize_sparql_queries(input_csv_path, output_csv_path):\n",
    "    global_var_counter = 1  # Initialize the global variable counter\n",
    "\n",
    "    with open(input_csv_path, 'r') as input_csv, open(output_csv_path, 'w', newline='') as output_csv:\n",
    "        csv_reader = csv.reader(input_csv)\n",
    "        csv_writer = csv.writer(output_csv)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue  # Skip empty rows\n",
    "\n",
    "            sparql_query = row[1]  # Assuming the SPARQL query is in the first column\n",
    "\n",
    "            # Extract variables from the SPARQL query using regular expressions\n",
    "            variables = re.findall(r'\\?([a-zA-Z_][a-zA-Z0-9_]*)', sparql_query)\n",
    "\n",
    "            # Create a mapping of old variables to new numerated variables\n",
    "            variable_mapping = {}\n",
    "            for variable in variables:\n",
    "                if variable not in variable_mapping:\n",
    "                    variable_mapping[variable] = f'?var{global_var_counter}'\n",
    "                    global_var_counter += 1\n",
    "\n",
    "            # Replace variables in the SPARQL query with numerated variables\n",
    "            for old_variable, new_variable in variable_mapping.items():\n",
    "                sparql_query = sparql_query.replace(f'?{old_variable}', new_variable)\n",
    "\n",
    "            # Write the normalized query to the output CSV\n",
    "            csv_writer.writerow([sparql_query])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_path = \"unique_normalised_12_9_2023.csv\"  # Replace with the path to your input CSV file\n",
    "    output_csv_path = \"normalized_queries.csv\"  # Replace with the desired output CSV file\n",
    "\n",
    "    normalize_sparql_queries(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdf7fe-5ec0-41fc-a9ac-f7a53c98bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the maximum column width to None to display the entire content of each cell\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv('normalized_queries.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f6a37-96e8-44b7-8ef1-81037e7fb698",
   "metadata": {},
   "source": [
    "get a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20e22256-7536-4134-b039-5f3769d67d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 rows have been saved to t.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_csv_file = 'find_bug_triples.csv'  # Replace with your input CSV file path\n",
    "output_csv_file = 't.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_csv_file)\n",
    "\n",
    "# Get the first 1000 rows, including the header\n",
    "sample_df = df.head(10 )\n",
    "\n",
    "# Save the sample DataFrame to an output CSV file\n",
    "sample_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"{len(sample_df)} rows have been saved to {output_csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dde436-96e1-4ff3-9a4a-a475203b534f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# get triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422c8fa-56f6-4fcc-9f1a-e619ca89fa1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Load your CSV data from the input file\n",
    "input_csv_file = 'parsed.csv'\n",
    "output_csv_file = 'find_bug_triples.csv'\n",
    "\n",
    "# Create a list to store the triples\n",
    "triples = []\n",
    "\n",
    "# Define a function to extract triples from the parse tree\n",
    "def extract_triples(parse_tree):\n",
    "    if \"triples\" in parse_tree:\n",
    "        for triple in parse_tree[\"triples\"]:\n",
    "            try:\n",
    "                subject = triple[\"subject\"][\"value\"]\n",
    "                predicate = triple[\"predicate\"][\"value\"]\n",
    "                obj = triple[\"object\"][\"value\"]\n",
    "                triples.append((subject, predicate, obj))\n",
    "            except KeyError as e:\n",
    "                print(f\"Error extracting triple: {e}\")\n",
    "                print(\"Offending triple:\", triple)\n",
    "    \n",
    "    # Recursively call the function on child nodes\n",
    "    for key, value in parse_tree.items():\n",
    "        if isinstance(value, dict):\n",
    "            extract_triples(value)\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    extract_triples(item)\n",
    "\n",
    "# Read the CSV file and process each row\n",
    "with open(input_csv_file, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        # Check if there is content in the second column and it doesn't contain the error message\n",
    "        if len(row) > 1 and row[1] and \"Error parsing the query\" not in row[1]:\n",
    "            # Extract triples from the content of the second column\n",
    "            content = row[1]\n",
    "            try:\n",
    "                parse_tree = json.loads(content)\n",
    "                extract_triples(parse_tree)\n",
    "            except json.JSONDecodeError:\n",
    "                # Handle invalid JSON if needed\n",
    "                pass\n",
    "\n",
    "# Write the extracted triples to the output CSV file\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['subject', 'predicate', 'object']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the triples\n",
    "    for triple in triples:\n",
    "        writer.writerow({'subject': triple[0], 'predicate': triple[1], 'object': triple[2]})\n",
    "\n",
    "print(\"Triples extracted and written to\", output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a5129a-006f-41c1-9845-f8af30cbe4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the input and output file names\n",
    "input_file = 'find_bug_triples.csv'\n",
    "output_file = 'filtered_data.csv'\n",
    "\n",
    "# Open the input data file for reading and the output file for writing\n",
    "with open(input_file, 'r') as input_file, open(output_file, 'w') as output_file:\n",
    "    for line in input_file:\n",
    "        # Split the line into components using tab (',') as the delimiter\n",
    "        components = line.strip().split(',')\n",
    "\n",
    "        # Check if the line has exactly three components (subject, predicate, object)\n",
    "        if len(components) == 3:\n",
    "            # Write complete triples to the output file\n",
    "            output_file.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c100c4-ff82-426e-ab3d-7f8c56d392f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Entity extraction from triples:\n",
    "if entity start with var do not write it in output file.\n",
    "\n",
    "if entity start with g_ do not write it in output file.\n",
    "\n",
    "if a entity has this substring \"nonsensical\" do not write it in output file.\n",
    "\n",
    "only write unique entities in output csv\n",
    "\n",
    "entities with spaces like are encoded to improve clustering \"DrugBank drugbank_vocabulary:Indication\"\n",
    "\n",
    "2177101 entities redeuced to 30918\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2769492e-e51b-4918-97b5-b325275810a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted entities have been saved to extracted_entities.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file_name = 'filtered_data.csv'\n",
    "output_file_name = 'extracted_entities.csv'\n",
    "\n",
    "subjects_and_objects = set()  # Using a set to store unique entities\n",
    "\n",
    "with open(input_file_name, mode ='r') as input_file:\n",
    "    csvReader = csv.reader(input_file)\n",
    "    \n",
    "    # Skip header if there's any\n",
    "    next(csvReader, None)\n",
    "\n",
    "    for row in csvReader:\n",
    "        # Assuming that the subject is the first element and object is the third element in the row\n",
    "        subject, predicate, object_ = row\n",
    "\n",
    "        if not subject.startswith('var') and not subject.startswith('g_') and 'nonsensical' not in subject:\n",
    "            subjects_and_objects.add(subject)\n",
    "            \n",
    "        if not object_.startswith('var') and not object_.startswith('g_') and 'nonsensical' not in object_:\n",
    "            subjects_and_objects.add(object_)\n",
    "\n",
    "# Write the extracted subjects and objects to an output CSV file\n",
    "with open(output_file_name, mode ='w', newline='') as output_file:\n",
    "    csvWriter = csv.writer(output_file)\n",
    "    \n",
    "    # Writing header\n",
    "    csvWriter.writerow(['Entity'])\n",
    "    \n",
    "    # Writing data\n",
    "    for entity in subjects_and_objects:\n",
    "        csvWriter.writerow([entity])\n",
    "\n",
    "print(f\"Extracted entities have been saved to {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8829c1d1-e55b-4f25-abe9-a523e8da09ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entity\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://purl.org/goodrelations/v1#Offering\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SLC9A4\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://bio2rdf.org/kegg:K03908\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>http://bio2rdf.org/wormbase:WBGene00198889\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>http://dbpedia.org/resource/Nortriptyline\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>SUCLG1\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>http://bio2rdf.org/go.ref_vocabulary:Resource\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>http://bio2rdf.org/wormbase:WBGene00017619\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0\n",
       "0                                          Entity\\r\n",
       "1                                                \\r\n",
       "2       http://purl.org/goodrelations/v1#Offering\\r\n",
       "3                                          SLC9A4\\r\n",
       "4                  http://bio2rdf.org/kegg:K03908\\r\n",
       "..                                              ...\n",
       "95     http://bio2rdf.org/wormbase:WBGene00198889\\r\n",
       "96      http://dbpedia.org/resource/Nortriptyline\\r\n",
       "97                                         SUCLG1\\r\n",
       "98  http://bio2rdf.org/go.ref_vocabulary:Resource\\r\n",
       "99     http://bio2rdf.org/wormbase:WBGene00017619\\r\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the maximum column width to None to display the entire content of each cell\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv('extracted_entities.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd5cc9-4c8d-4cb1-9f99-8991e54fac24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the clusters and the entities belonging to each cluster\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(df[df['cluster'] == i]['Entity'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf11e4-e6c7-47d1-95b0-7ccdb65b7af4",
   "metadata": {},
   "source": [
    "# Find optimal number of clusters:\n",
    "Elbow Method\n",
    "picking the \"elbow\" of the curve as the number of clusters to use. The idea is to choose a small value of k (number of clusters) that still has a low sum of squared distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c794194-f978-47c7-b22b-86067a396336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply KMeans with the chosen number of clusters\n",
    "optimal_clusters = 31  # replace this with the number of clusters determined by the Elbow Method\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Print the clusters and the entities belonging to each cluster\n",
    "for i in range(optimal_clusters):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(df[df['cluster'] == i]['Entity'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ec26c-926a-49b0-a59b-6ff2bcb13ee6",
   "metadata": {},
   "source": [
    "# query type of entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d803c-66df-4ec1-8010-1335c51e287c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install SPARQLWrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e3988-77b7-41e4-9c9d-147fb5c96989",
   "metadata": {},
   "source": [
    "# get types but if the entity is a type return itself and other types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be7276f9-6cac-403c-b6f4-ba2831b6e817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    " \n",
    "sparql_endpoint = \"http://bio2rdf.org/sparql\"\n",
    "\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "def is_url(string):\n",
    "    url_regex = re.compile(\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    )\n",
    "    return re.match(url_regex, string) is not None\n",
    "\n",
    "def execute_query_and_write_results(entity, query, is_entity_url):\n",
    "    try:\n",
    "        sparql.setQuery(query)\n",
    "        results = sparql.query().convert()\n",
    "        types = set()\n",
    "        has_results = bool(results[\"results\"][\"bindings\"])  # Check if the result set is not empty\n",
    "\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            type_value = result.get(\"type\", {}).get(\"value\")\n",
    "            \n",
    "            if type_value:\n",
    "                if type_value in [\"http://www.w3.org/2002/07/owl#Class\", \"http://www.w3.org/2000/01/rdf-schema#Class\"]:\n",
    "                    types.add(entity)\n",
    "                else:\n",
    "                    types.add(type_value)\n",
    " \n",
    "        if has_results and not is_entity_url:\n",
    "            titles_file.write(entity + '\\n')\n",
    "\n",
    "        for type_value in types:\n",
    "            types_file.write(type_value + '\\n')\n",
    "\n",
    "        if not types:\n",
    "            without_type_file.write(entity + '\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the entity {entity}: {e}\")\n",
    "\n",
    "with open('extracted_entities.csv', mode ='r') as file, \\\n",
    "     open('types1.txt', 'w') as types_file, \\\n",
    "     open('titles1.txt', 'w') as titles_file, \\\n",
    "     open('without_type1.txt', 'w') as without_type_file:\n",
    "\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        entity = row[0].strip()\n",
    "        is_entity_url = is_url(entity)\n",
    "\n",
    "        if is_entity_url:\n",
    "            query = f\"\"\"\n",
    "                PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "                PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "                SELECT DISTINCT ?type  \n",
    "                WHERE {{\n",
    "                    <{entity}> rdf:type ?type .\n",
    "                                    }}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            query = f\"\"\"\n",
    "                PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "                PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "                SELECT ?type \n",
    "                WHERE {{ \n",
    "                       ?s dcterms:title \"{entity}\" ;\n",
    "                                rdf:type ?type .\n",
    "                }}\n",
    "            \"\"\"\n",
    "\n",
    "        execute_query_and_write_results(entity, query, is_entity_url)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2d875-46af-4c81-8524-7953c912d6a7",
   "metadata": {},
   "source": [
    "30918 entities reduced to 1030 types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fee555-0670-47e4-9b52-868ffc61e296",
   "metadata": {},
   "source": [
    "## properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee05bbf-77fa-4f1f-8593-93316084b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# List of graphs\n",
    "graphs = [\n",
    "\"http://bio2rdf.org/sgd_resource:bio2rdf.dataset.sgd.R3\",\n",
    "\"http://bio2rdf.org/taxonomy_resource:bio2rdf.dataset.taxonomy.R3\",\n",
    "\"http://bio2rdf.org/homologene_resource:bio2rdf.dataset.homologene.R3\",\n",
    "\"http://bio2rdf.org/interpro_resource:bio2rdf.dataset.interpro.R3\",\n",
    "\"http://bio2rdf.org/bioportal_resource:bio2rdf.dataset.bioportal.R3\",\n",
    "\"http://bio2rdf.org/clinicaltrials_resource:bio2rdf.dataset.clinicaltrials.R3\",\n",
    "\"http://bio2rdf.org/kegg_resource:bio2rdf.dataset.kegg.R3\",\n",
    "\"http://bio2rdf.org/pharmgkb_resource:bio2rdf.dataset.pharmgkb.R3\",\n",
    "\"http://bio2rdf.org/hgnc_resource:bio2rdf.dataset.hgnc.R3\",\n",
    "\"http://bio2rdf.org/mesh_resource:bio2rdf.dataset.mesh.R3\",\n",
    "\"http://bio2rdf.org/omim_resource:bio2rdf.dataset.omim.R3\",\n",
    "\"http://bio2rdf.org/sider_resource:bio2rdf.dataset.sider.R3\",\n",
    "\"http://bio2rdf.org/apo_resource:bio2rdf.dataset.apo.R3\",\n",
    "\"http://bio2rdf.org/ctd_resource:bio2rdf.dataset.ctd.R3\",\n",
    "\"http://bio2rdf.org/go_resource:bio2rdf.dataset.go.R3\",\n",
    "\"http://bio2rdf.org/hp_resource:bio2rdf.dataset.hp.R3\",\n",
    "\"http://bio2rdf.org/drugbank_resource:bio2rdf.dataset.drugbank.R3\",\n",
    "\"http://bio2rdf.org/mgi_resource:bio2rdf.dataset.mgi.R3\",\n",
    "\"http://bio2rdf.org/goa_resource:bio2rdf.dataset.goa.R3\",\n",
    "\"http://bio2rdf.org/ndc_resource:bio2rdf.dataset.ndc.R3\",\n",
    "\"http://bio2rdf.org/wormbase_resource:bio2rdf.dataset.wormbase.R3\",\n",
    "\"http://bio2rdf.org/lsr_resource:bio2rdf.dataset.lsr.R3\",\n",
    "\"http://bio2rdf.org/affymetrix_resource:bio2rdf.dataset.affymetrix.R3\",\n",
    "\"http://bio2rdf.org/ncbigene_resource:bio2rdf.dataset.ncbigene.R3\",\n",
    "\"http://bio2rdf.org/eco_resource:bio2rdf.dataset.eco.R3\",\n",
    "\"http://bio2rdf.org/irefindex_resource:bio2rdf.dataset.irefindex.R3\"]\n",
    "\n",
    "# Function to check if predicate exists in a specific graph of Bio2RDF\n",
    "def check_predicate_in_graph(predicate, graph):\n",
    "    sparql = SPARQLWrapper(\"https://bio2rdf.org/sparql\")\n",
    "    query = f\"ASK FROM <{graph}> {{ ?s <{predicate}> ?o }}\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        return results['boolean']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying predicate {predicate} in graph {graph}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Lists to hold predicates based on existence in Bio2RDF and errors\n",
    "exists = []\n",
    "does_not_exist = []\n",
    "errors = []\n",
    "\n",
    "# Read predicates from CSV file and check their existence in each Bio2RDF graph\n",
    "with open('Logs_properties.csv', mode='r') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "    for predicate in csvFile:\n",
    "        predicate_found = False\n",
    "        for graph in graphs:\n",
    "            result = check_predicate_in_graph(predicate[0], graph)\n",
    "            if result is True:\n",
    "                exists.append(predicate[0])\n",
    "                predicate_found = True\n",
    "                break\n",
    "            elif result is None:\n",
    "                errors.append(predicate[0])\n",
    "                break\n",
    "        if not predicate_found and predicate[0] not in errors:\n",
    "            does_not_exist.append(predicate[0])\n",
    "\n",
    "# Write predicates that exist in Bio2RDF to a file\n",
    "with open('10_Logs_properties.csv', 'w') as file:\n",
    "    for predicate in exists:\n",
    "        file.write(predicate + '\\n')\n",
    "\n",
    "# Write predicates that do not exist in Bio2RDF to a file\n",
    "with open('10_p_not_in_bio2rdf.txt', 'w') as file:\n",
    "    for predicate in does_not_exist:\n",
    "        file.write(predicate + '\\n')\n",
    "\n",
    "# Print or log predicates that caused errors\n",
    "print(\"Predicates that caused errors:\")\n",
    "for predicate in errors:\n",
    "    print(predicate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd9093-6ca3-45c3-ba4f-017129e66c49",
   "metadata": {},
   "source": [
    "# Get Schema vocabularies from KG using queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5503a542-2a1d-4bc2-a972-9aac3993bf6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing http://bio2rdf.org/sgd_resource:bio2rdf.dataset.sgd.R3\n",
      "Query returned 92 results.\n",
      "Query returned 74 results.\n",
      "Processing http://bio2rdf.org/taxonomy_resource:bio2rdf.dataset.taxonomy.R3\n",
      "Query returned 11 results.\n",
      "Query returned 51 results.\n",
      "Processing http://bio2rdf.org/homologene_resource:bio2rdf.dataset.homologene.R3\n",
      "Query returned 12 results.\n",
      "Query returned 24 results.\n",
      "Processing http://bio2rdf.org/interpro_resource:bio2rdf.dataset.interpro.R3\n",
      "Query returned 2 results.\n",
      "Query returned 13 results.\n",
      "Processing http://bio2rdf.org/bioportal_resource:bio2rdf.dataset.bioportal.R3\n",
      "Query returned 5 results.\n",
      "Query returned 35 results.\n",
      "Processing http://bio2rdf.org/clinicaltrials_resource:bio2rdf.dataset.clinicaltrials.R3\n",
      "Query returned 65 results.\n",
      "Query returned 168 results.\n",
      "Processing http://bio2rdf.org/kegg_resource:bio2rdf.dataset.kegg.R3\n",
      "Query returned 77 results.\n",
      "Query returned 150 results.\n",
      "Processing http://bio2rdf.org/pharmgkb_resource:bio2rdf.dataset.pharmgkb.R3\n",
      "Query returned 68 results.\n",
      "Query returned 85 results.\n",
      "Processing http://bio2rdf.org/hgnc_resource:bio2rdf.dataset.hgnc.R3\n",
      "Query returned 23 results.\n",
      "Query returned 52 results.\n",
      "Processing http://bio2rdf.org/mesh_resource:bio2rdf.dataset.mesh.R3\n",
      "Query returned 14 results.\n",
      "Query returned 71 results.\n",
      "Processing http://bio2rdf.org/omim_resource:bio2rdf.dataset.omim.R3\n",
      "Query returned 40 results.\n",
      "Query returned 103 results.\n",
      "Processing http://bio2rdf.org/sider_resource:bio2rdf.dataset.sider.R3\n",
      "Query returned 12 results.\n",
      "Query returned 25 results.\n",
      "Processing http://bio2rdf.org/apo_resource:bio2rdf.dataset.apo.R3\n",
      "Query returned 10 results.\n",
      "Query returned 31 results.\n",
      "Processing http://bio2rdf.org/ctd_resource:bio2rdf.dataset.ctd.R3\n",
      "Query returned 27 results.\n",
      "Query returned 40 results.\n",
      "Processing http://bio2rdf.org/go_resource:bio2rdf.dataset.go.R3\n",
      "Query returned 37 results.\n",
      "Query returned 83 results.\n",
      "Processing http://bio2rdf.org/hp_resource:bio2rdf.dataset.hp.R3\n",
      "Query returned 23 results.\n",
      "Query returned 57 results.\n",
      "Processing http://bio2rdf.org/drugbank_resource:bio2rdf.dataset.drugbank.R3\n",
      "Query returned 103 results.\n",
      "Query returned 113 results.\n",
      "Processing http://bio2rdf.org/mgi_resource:bio2rdf.dataset.mgi.R3\n",
      "Query returned 31 results.\n",
      "Query returned 48 results.\n",
      "Processing http://bio2rdf.org/goa_resource:bio2rdf.dataset.goa.R3\n",
      "Query returned 15 results.\n",
      "Query returned 36 results.\n",
      "Processing http://bio2rdf.org/ndc_resource:bio2rdf.dataset.ndc.R3\n",
      "Query returned 18 results.\n",
      "Query returned 37 results.\n",
      "Processing http://bio2rdf.org/wormbase_resource:bio2rdf.dataset.wormbase.R3\n",
      "Query returned 48 results.\n",
      "Query returned 46 results.\n",
      "Processing http://bio2rdf.org/lsr_resource:bio2rdf.dataset.lsr.R3\n",
      "Query returned 15 results.\n",
      "Query returned 38 results.\n",
      "Processing http://bio2rdf.org/affymetrix_resource:bio2rdf.dataset.affymetrix.R3\n",
      "Query returned 45 results.\n",
      "Query returned 83 results.\n",
      "Processing http://bio2rdf.org/ncbigene_resource:bio2rdf.dataset.ncbigene.R3\n",
      "Query returned 37 results.\n",
      "Query returned 67 results.\n",
      "Processing http://bio2rdf.org/eco_resource:bio2rdf.dataset.eco.R3\n",
      "Query returned 13 results.\n",
      "Query returned 42 results.\n",
      "Processing http://bio2rdf.org/irefindex_resource:bio2rdf.dataset.irefindex.R3\n",
      "Query returned 84 results.\n",
      "Query returned 47 results.\n",
      "Wrote 601 items to 10_Classes_schema.csv\n",
      "Wrote 1106 items to 10_Predicates_schema.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import csv\n",
    "\n",
    "# List of dataset files\n",
    "dataset_files = [\n",
    "\"http://bio2rdf.org/sgd_resource:bio2rdf.dataset.sgd.R3\",\n",
    "\"http://bio2rdf.org/taxonomy_resource:bio2rdf.dataset.taxonomy.R3\",\n",
    "\"http://bio2rdf.org/homologene_resource:bio2rdf.dataset.homologene.R3\",\n",
    "\"http://bio2rdf.org/interpro_resource:bio2rdf.dataset.interpro.R3\",\n",
    "\"http://bio2rdf.org/bioportal_resource:bio2rdf.dataset.bioportal.R3\",\n",
    "\"http://bio2rdf.org/clinicaltrials_resource:bio2rdf.dataset.clinicaltrials.R3\",\n",
    "\"http://bio2rdf.org/kegg_resource:bio2rdf.dataset.kegg.R3\",\n",
    "\"http://bio2rdf.org/pharmgkb_resource:bio2rdf.dataset.pharmgkb.R3\",\n",
    "\"http://bio2rdf.org/hgnc_resource:bio2rdf.dataset.hgnc.R3\",\n",
    "\"http://bio2rdf.org/mesh_resource:bio2rdf.dataset.mesh.R3\",\n",
    "\"http://bio2rdf.org/omim_resource:bio2rdf.dataset.omim.R3\",\n",
    "\"http://bio2rdf.org/sider_resource:bio2rdf.dataset.sider.R3\",\n",
    "\"http://bio2rdf.org/apo_resource:bio2rdf.dataset.apo.R3\",\n",
    "\"http://bio2rdf.org/ctd_resource:bio2rdf.dataset.ctd.R3\",\n",
    "\"http://bio2rdf.org/go_resource:bio2rdf.dataset.go.R3\",\n",
    "\"http://bio2rdf.org/hp_resource:bio2rdf.dataset.hp.R3\",\n",
    "\"http://bio2rdf.org/drugbank_resource:bio2rdf.dataset.drugbank.R3\",\n",
    "\"http://bio2rdf.org/mgi_resource:bio2rdf.dataset.mgi.R3\",\n",
    "\"http://bio2rdf.org/goa_resource:bio2rdf.dataset.goa.R3\",\n",
    "\"http://bio2rdf.org/ndc_resource:bio2rdf.dataset.ndc.R3\",\n",
    "\"http://bio2rdf.org/wormbase_resource:bio2rdf.dataset.wormbase.R3\",\n",
    "\"http://bio2rdf.org/lsr_resource:bio2rdf.dataset.lsr.R3\",\n",
    "\"http://bio2rdf.org/affymetrix_resource:bio2rdf.dataset.affymetrix.R3\",\n",
    "\"http://bio2rdf.org/ncbigene_resource:bio2rdf.dataset.ncbigene.R3\",\n",
    "\"http://bio2rdf.org/eco_resource:bio2rdf.dataset.eco.R3\",\n",
    "\"http://bio2rdf.org/irefindex_resource:bio2rdf.dataset.irefindex.R3\"]\n",
    "\n",
    "# Replace `your_sparql_endpoint` with the actual SPARQL endpoint URL\n",
    "sparql_endpoint = \"http://bio2rdf.org/sparql\"\n",
    "def execute_query(sparql_query):\n",
    "    sparql = SPARQLWrapper(sparql_endpoint)\n",
    "    sparql.setQuery(sparql_query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        print(f\"Query returned {len(results['results']['bindings'])} results.\")  # Diagnostic print\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# This will hold all distinct class types and predicates\n",
    "classes_set = set()\n",
    "predicates_set = set()\n",
    "\n",
    "# Function to write results to CSV\n",
    "def write_results_to_csv(file_name, results_set):\n",
    "    with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for item in results_set:\n",
    "            writer.writerow([item])\n",
    "        print(f\"Wrote {len(results_set)} items to {file_name}\")  # Diagnostic print\n",
    "\n",
    "# Loop through dataset files to construct the schema\n",
    "for file_url in dataset_files:\n",
    "    # dataset_uri = extract_dataset_uri(file_url)\n",
    "    # if dataset_uri:\n",
    "        print(f\"Processing {file_url}\")\n",
    "        # Query to find all distinct types\n",
    "        query_types = f\"\"\"\n",
    "        SELECT DISTINCT ?type\n",
    "        FROM <{file_url}>\n",
    "        WHERE {{ ?s a ?type }}\n",
    "        \"\"\"\n",
    "        results_types = execute_query(query_types)\n",
    "        for result in results_types[\"results\"][\"bindings\"]:\n",
    "            classes_set.add(result[\"type\"][\"value\"])\n",
    "        \n",
    "        # Query to find all distinct predicates\n",
    "        query_predicates = f\"\"\"\n",
    "        SELECT DISTINCT ?p\n",
    "        FROM <{file_url}>\n",
    "        WHERE {{ ?s ?p ?o }}\n",
    "        \"\"\"\n",
    "        results_predicates = execute_query(query_predicates)\n",
    "        for result in results_predicates[\"results\"][\"bindings\"]:\n",
    "            predicates_set.add(result[\"p\"][\"value\"])\n",
    "        \n",
    "# Write the results to CSV files\n",
    "write_results_to_csv('10_Classes_schema.csv', classes_set)\n",
    "write_results_to_csv('10_Predicates_schema.csv', predicates_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610c57e-de25-4ce1-baaf-38efc5cb98f8",
   "metadata": {},
   "source": [
    "# Get superclasses as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b52c1532-53ac-4fea-8876-4b21b7d2b3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Initialize the SPARQL wrapper with the endpoint URL\n",
    "sparql = SPARQLWrapper(\"http://bio2rdf.org/sparql\")\n",
    "\n",
    "# Function to get the superclass of a given class\n",
    "def get_superclass(class_uri):\n",
    "    query = \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT DISTINCT ?superclass\n",
    "    WHERE {\n",
    "      <\"\"\" + class_uri + \"\"\"> rdfs:subClassOf ?superclass .\n",
    "    }\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    # Extract the superclass URIs from the query results\n",
    "    superclasses = [result['superclass']['value'] for result in results[\"results\"][\"bindings\"]]\n",
    "    # print(\"s\")\n",
    "    # print(\"superclasses\", superclasses)\n",
    "    return superclasses\n",
    "\n",
    "# Read the classes from the CSV file\n",
    "classes_df = pd.read_csv('10_superclasses.csv')\n",
    "\n",
    "# This assumes that the first column contains the class URIs\n",
    "class_uris = classes_df.iloc[:, 0].unique()\n",
    "\n",
    "# Get the superclasses for each class URI\n",
    "superclasses_list = []\n",
    "for class_uri in class_uris:\n",
    "    superclasses = get_superclass(class_uri)\n",
    "    superclasses_list.extend(superclasses)\n",
    "\n",
    "# Remove duplicates and write to CSV\n",
    "superclasses_df = pd.DataFrame(list(set(superclasses_list)), columns=['Superclass'])\n",
    "superclasses_df.to_csv('10_sup_superclasses.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb80a9-167a-424e-a1a4-c9c13dc37776",
   "metadata": {},
   "source": [
    "# Get subclasses as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3cadc-21b9-43ec-b737-cf3f73281467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74981ad6-3233-40d9-94f4-f905eae2f02c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "superclasses ['http://bio2rdf.org/wormbase_vocabulary:Change_of_expression_level-Regulatory-Interaction', 'http://bio2rdf.org/wormbase_vocabulary:Regulatory-Interaction', 'http://bio2rdf.org/wormbase_vocabulary:Change_of_localization-Regulatory-Interaction']\n",
      "s\n",
      "superclasses []\n",
      "s\n",
      "superclasses []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Initialize the SPARQL wrapper with the endpoint URL\n",
    "sparql = SPARQLWrapper(\"http://bio2rdf.org/sparql\")\n",
    "\n",
    "# Function to get the superclass of a given class\n",
    "def get_superclass(class_uri):\n",
    "    query = \"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT DISTINCT ?subclass\n",
    "    WHERE {\n",
    "      ?subclass rdfs:subClassOf <\"\"\" + class_uri + \"\"\"> .\n",
    "    }\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    # Extract the superclass URIs from the query results\n",
    "    superclasses = [result['subclass']['value'] for result in results[\"results\"][\"bindings\"]]\n",
    "    # print(\"s\")\n",
    "    # print(\"superclasses\", superclasses)\n",
    "    return superclasses\n",
    "\n",
    "# Read the classes from the CSV file\n",
    "classes_df = pd.read_csv('10_subclass.csv')\n",
    "\n",
    "# This assumes that the first column contains the class URIs\n",
    "class_uris = classes_df.iloc[:, 0].unique()\n",
    "\n",
    "# Get the superclasses for each class URI\n",
    "superclasses_list = []\n",
    "for class_uri in class_uris:\n",
    "    superclasses = get_superclass(class_uri)\n",
    "    superclasses_list.extend(superclasses)\n",
    "\n",
    "# Remove duplicates and write to CSV\n",
    "superclasses_df = pd.DataFrame(list(set(superclasses_list)), columns=['Superclass'])\n",
    "superclasses_df.to_csv('10_sub_subclass.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed59553-2d79-4508-9f18-87221ad37ca1",
   "metadata": {
    "tags": []
   },
   "source": [
    " ##  clean schema classes [KG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "69619d7a-23e2-4f39-b633-bb89d94dd085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows have been written to 10_bio2rdf_spesific_Classes_schema.csv as per the conditions.\n",
      "count is: 527\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the input and output filenames\n",
    "input_filename = '10_Classes_schema.csv'\n",
    "output_filename = '10_bio2rdf_spesific_Classes_schema.csv'\n",
    "count = 0\n",
    "# Open the input file for reading and the output file for writing\n",
    "with open(input_filename, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "        open(output_filename, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "    # Create reader and writer objects\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # Iterate over each row in the input CSV\n",
    "    for row in reader:\n",
    "        # Check each cell in the row\n",
    "        for value in row:\n",
    "            # If the cell contains '_vocabulary' or contains 'http:' without ':', write the row to the output CSV\n",
    "#             or ('http:' in value and ':' not in value.replace('http:', ''))\n",
    "            if '_vocabulary' in value :\n",
    "                writer.writerow(row)\n",
    "                count = count + 1\n",
    "                break  # We write the row once if it meets the condition and then move to the next row\n",
    "\n",
    "print(f\"Rows have been written to {output_filename} as per the conditions.\")\n",
    "print(\"count is:\", count )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c415f-9fd6-4088-a70d-96f858baf1c9",
   "metadata": {
    "tags": []
   },
   "source": [
    " ##  clean types extracted from queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a15c247b-7c88-40d9-9db4-41227d9cea1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows have been written to types_not_domain.csv as per the conditions.\n",
      "count is: 79\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the input and output filenames\n",
    "input_filename = 'types1.csv'\n",
    "output_filename = 'types_not_domain.csv'\n",
    "count = 0\n",
    "# Open the input file for reading and the output file for writing\n",
    "with open(input_filename, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "        open(output_filename, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "    # Create reader and writer objects\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # Iterate over each row in the input CSV\n",
    "    for row in reader:\n",
    "        # Check each cell in the row\n",
    "        for value in row:\n",
    "            # If the cell contains '_vocabulary' or contains 'http:' without ':', write the row to the output CSV\n",
    "#             or ('http:' in value and ':' not in value.replace('http:', ''))\n",
    "            if '_vocabulary' not in value and ('http:' in value and ':' not in value.replace('http:', '')):\n",
    "                writer.writerow(row)\n",
    "                count = count + 1\n",
    "                break  # We write the row once if it meets the condition and then move to the next row\n",
    "\n",
    "print(f\"Rows have been written to {output_filename} as per the conditions.\")\n",
    "print(\"count is:\", count )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41640bee-f3bb-4718-8b4e-b3ef1c6ff7c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## get the entities that are in kg schema but not in queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf9707-0d97-419b-9a57-f4eb98247e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " import csv\n",
    "\n",
    "# Step 1: Read entities from `u_types1_labels.csv`\n",
    "with open(\"10_bio2rdf_total_schema_elements.txt\", \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    entities = [row[0] for row in csv_reader]  # assuming the entity is in the first column\n",
    "\n",
    "# Step 2: Check if they exist in `1u-labels-u-kg-types1.txt`\n",
    "with open(\"types20.csv\", \"r\") as txt_file:\n",
    "    txt_content = txt_file.read().splitlines()\n",
    "\n",
    "entities_in_kg = [entity for entity in entities if entity in txt_content]\n",
    "entities_not_in_kg = [entity for entity in entities if entity not in txt_content]\n",
    "\n",
    "# Step 3: Write the existing entities to `EntitiesInQueriesExist_in_KG_schema.csv`\n",
    "with open(\"typesInQueriesExist_in_KG_schema.csv\", \"w\", newline='') as csv_output:\n",
    "    csv_writer = csv.writer(csv_output)\n",
    "    for entity in entities_in_kg:\n",
    "        csv_writer.writerow([entity])\n",
    "\n",
    "# Step 4: Print the count and the entities that don't exist\n",
    "print(f\"Count of entities in queries that exist in KG schema: {len(entities_not_in_kg)}\")\n",
    "print(\"\\nEntities that don't exist in KG schema:\")\n",
    "for entity in entities_not_in_kg:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d41090-62f9-47f9-9c0e-a9586bfa5804",
   "metadata": {},
   "source": [
    "\n",
    "## get the entities that are in queries but not in kg schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae001680-a3d5-4ebf-a6eb-8ea5533ff1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of entities in queries that not exist in KG schema: 4\n",
      "\n",
      "Entities that don't exist in KG schema:\n",
      "http://bio2rdf.org/hgnc_vocabulary:Status\n",
      "http://bio2rdf.org/drugbank_vocabulary:7cab3885cdbcb9df8c405e9c9ad10732\n",
      "http://bio2rdf.org/hgnc_vocabulary:Approved\n",
      "http://bio2rdf.org/atmo_vocabulary:Resource\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Step 1: Read entities from `u_types1_labels.csv`\n",
    "with open(\"types20.csv\", \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    entities = [row[0] for row in csv_reader]  # assuming the entity is in the first column\n",
    "\n",
    "# Step 2: Check if they exist in `1u-labels-u-kg-types1.txt`\n",
    "with open( \"10_sup_bio2rdf_spesific_Classes_schema.csv\" , \"r\") as txt_file:\n",
    "    txt_content = txt_file.read().splitlines()\n",
    "\n",
    "entities_in_kg = [entity for entity in entities if entity in txt_content]\n",
    "entities_not_in_kg = [entity for entity in entities if entity not in txt_content]\n",
    "\n",
    "# Step 3: Write the existing entities to `EntitiesInQueriesExist_in_KG_schema.csv`\n",
    "with open(\"queries_types_valid_KG_schema.csv\", \"w\", newline='') as csv_output:\n",
    "    csv_writer = csv.writer(csv_output)\n",
    "    for entity in entities_in_kg:\n",
    "        csv_writer.writerow([entity])\n",
    "\n",
    "# Step 4: Print the count and the entities that don't exist\n",
    "print(f\"Count of entities in queries that not exist in KG schema: {len(entities_not_in_kg)}\")\n",
    "print(\"\\nEntities that don't exist in KG schema:\")\n",
    "for entity in entities_not_in_kg:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a9954-f749-4931-946e-1592ffc4f3c0",
   "metadata": {},
   "source": [
    " ##  clean properties extracted from queries [Logs_properties.csv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546f287a-6681-4c67-93c4-3aca488a992e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of properties in queries that not exist in KG schema: 198\n",
      "\n",
      " properties that don't exist in KG schema:\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftXmlIndex\n",
      "http://purl.org/goodrelations/v1#includes\n",
      "http://www.w3.org/ns/dcat#downloadURL\n",
      "http://semanticscience.org/resource/SIO_000205\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSuperFormats\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfCmpFuncName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvcAlias\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfMapsOnlyNullToNull\n",
      "http://www.openlinksw.com/schemas/virtrdf#qsMatchingFlags\n",
      "http://www.w3.org/ns/dcat#theme\n",
      "http://www.w3.org/2000/01/rdf-schema#domain\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSparqlEbvTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIidOfShortTmpl\n",
      "http://purl.org/dc/terms/accrualPeriodicity\n",
      "http://purl.org/dc/terms/hasPart\n",
      "http://rdfs.org/ns/void#propertyPartition\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortTmpl\n",
      "http://semanticscience.org/resource/SIO_000008\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_5\n",
      "http://rdfs.org/ns/void#exampleResource\n",
      "http://rdfs.org/ns/void#vocabulary\n",
      "http://www.w3.org/ns/sparql-service-description#feature\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfStrsqlvalOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfValRange-rvrLanguage\n",
      "http://www.bigdata.com/rdf#/features/KB/Namespace\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_2\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftColumnName\n",
      "http://purl.org/ontology/wi/core#evidence\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfName\n",
      "http://xmlns.com/foaf/0.1/logo\n",
      "http://www.w3.org/2000/01/rdf-schema#type\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfExistingShortOfUriTmpl\n",
      "http://purl.org/goodrelations/v1#acceptedPaymentMethods\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvTableName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmPredicateMap\n",
      "http://purl.org/pav/retrievedFrom\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfDtpOfNiceSqlval\n",
      "http://semanticscience.org/resource/SIO_010078\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvFText\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftAlias\n",
      "http://rdfs.org/ns/void#subjectsTarget\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsSubformatOfLongWhenEqToSql\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfTypedsqlvalTmpl\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_4\n",
      "http://purl.org/dc/terms/extent\n",
      "http://www.openlinksw.com/schemas/virtrdf#inheritFrom\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_3\n",
      "http://www.openlinksw.com/schemas/virtrdf#item\n",
      "http://rdfs.org/ns/void#sparqlEndpoint\n",
      "http://www.w3.org/2002/07/owl#complementOf\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_1\n",
      "http://semanticscience.org/resource/SIO_000341\n",
      "http://www.w3.org/2000/01/rdf-schema#range\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsuriOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsBijection\n",
      "http://purl.org/pav/createdWith\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#TEL\n",
      "http://xmlns.com/foaf/0.1/maker\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmTableName\n",
      "http://purl.org/goodrelations/v1#hasPriceSpecification\n",
      "http://rdfs.org/ns/void#properties\n",
      "http://purl.org/pav/authoredOn\n",
      "http://www.openlinksw.com/schemas/virtrdf#qsUserMaps\n",
      "http://purl.org/goodrelations/v1#validFrom\n",
      "http://www.w3.org/2000/01/rdf-schema#isDescribedUsing\n",
      "http://rdfs.org/ns/void#uriSpace\n",
      "http://purl.org/goodrelations/v1#amountOfThisGood\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#Street\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#ADR\n",
      "http://purl.org/dc/terms/conformsTo\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvFormat\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfLongTmpl\n",
      "http://rdfs.org/ns/void#distinctSubjects\n",
      "http://www.w3.org/ns/sparql-service-description#endpoint\n",
      "http://www.openlinksw.com/schemas/virtrdf#isSpecialPredicate\n",
      "http://purl.org/dc/terms/references\n",
      "http://purl.org/pav/importedOn\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfLanguageTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfBoolOfShortTmpl\n",
      "http://semanticscience.org/resource/SIO_000772\n",
      "http://www.w3.org/ns/sparql-service-description#defaultDataset\n",
      "http://purl.org/goodrelations/v1#validThrough\n",
      "http://purl.org/goodrelations/v1#hasBusinessFunction\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsrefOfShortTmpl\n",
      "http://purl.org/dc/terms/issued\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfTypemaxTmpl\n",
      "http://semanticscience.org/resource/SIO_000062\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSqlvalOfShortTmpl\n",
      "http://purl.uniprot.org/core/organism\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSparqlEbvOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsnumericOfShortTmpl\n",
      "http://semanticscience.org/resource/SIO_000095\n",
      "http://semanticscience.org/resource/SIO_000300\n",
      "http://purl.org/dc/terms/createdBy\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIslitOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsStable\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfExistingShortOfSqlvalTmpl\n",
      "http://purl.org/goodrelations/v1#BusinessEntity\n",
      "http://www.w3.org/2004/02/skos/core#exactMatch\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfExistingShortOfLongTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#loadAs\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfColumnCount\n",
      "http://purl.org/goodrelations/v1#availableAtOrFrom\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvColumns\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfLanguageOfShortTmpl\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#City\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvaAlias\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfWrapDistinct\n",
      "http://purl.org/goodrelations/v1#hasUnitOfMeasurement\n",
      "http://www.openlinksw.com/schemas/virtrdf#noInherit\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSqlvalTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmf01uriOfShortTmpl\n",
      "http://purl.org/dc/terms/language\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsblankOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfLongTmpl\n",
      "http://rdfs.org/ns/void#linkPredicate\n",
      "http://www.openlinksw.com/schemas/virtrdf#dialect\n",
      "http://purl.org/dc/terms/isVersionOf\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmMatchingFlags\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#EMAIL\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmGraphMap\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfLongOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfSqlvalTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsSubformatOfLongWhenRef\n",
      "http://semanticscience.org/resource/SIO_000628\n",
      "http://www.w3.org/ns/sparql-service-description#defaultGraph\n",
      "http://purl.org/goodrelations/v1#availableDeliveryMethods\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvATables\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvaTableName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfCustomString1\n",
      "http://purl.org/pav/authoredBy\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmObjectMap\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvcColumnName\n",
      "http://purl.org/pav/version\n",
      "http://rdfs.org/ns/void#objectsTarget\n",
      "http://rdfs.org/ns/void#entities\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmSubjectMap\n",
      "http://purl.org/pav/createdBy\n",
      "http://semanticscience.org/resource/SIO_000253\n",
      "http://purl.org/goodrelations/v1#includesObject\n",
      "http://purl.org/goodrelations/v1#eligibleRegions\n",
      "http://purl.org/goodrelations/v1#legalName\n",
      "http://purl.org/goodrelations/v1#eligibleCustomerTypes\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#Pcode\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfExistingShortOfTypedsqlvalTmpl\n",
      "http://www.w3.org/2002/07/owl#versionInfo\n",
      "http://rdfs.org/ns/void#classPartition\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmf01blankOfShortTmpl\n",
      "http://purl.org/dc/terms/rightsHolder\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfValRange-rvrDatatype\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfUriTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfValRange-rvrRestrictions\n",
      "http://purl.org/pav/2.0/version\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfUriOfShortTmpl\n",
      "http://rdfs.org/ns/void#class\n",
      "http://purl.org/pav/lastUpdateOn\n",
      "http://www.w3.org/ns/sparql-service-description#supportedLanguage\n",
      "http://www.w3.org/ns/sparql-service-description#url\n",
      "http://rdfs.org/ns/void#triples\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsSubformatOfLong\n",
      "http://xmlns.com/foaf/0.1/homepage\n",
      "http://xmlns.com/foaf/0.1/mbox\n",
      "http://www.openlinksw.com/schemas/virtrdf#version\n",
      "http://rdfs.org/ns/void#property\n",
      "http://purl.org/goodrelations/v1#offers\n",
      "http://www.openlinksw.com/schemas/DAV#ownerUser\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvGeo\n",
      "http://www.w3.org/ns/sparql-service-description#resultFormat\n",
      "http://www.w3.org/2002/07/owl#priorVersion\n",
      "http://semanticscience.org/resource/SIO_000001\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfDatatypeTmpl\n",
      "http://purl.org/pav/createdOn\n",
      "http://xmlns.com/foaf/0.1/primaryTopic\n",
      "http://www.openlinksw.com/schemas/virtrdf#catName\n",
      "http://rdfs.org/ns/void#classes\n",
      "http://www.w3.org/ns/prov#wasDerivedFrom\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfOkForAnySqlvalue\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfBoolTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#isGcResistantType\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#Country\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftTableName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qsDefaultMap\n",
      "http://rdfs.org/ns/void#distinctObjects\n",
      "http://semanticscience.org/resource/SIO_000216\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvColumnsFormKey\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfTypeminTmpl\n",
      "http://www.w3.org/ns/sparql-service-description#inputFormat\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfDatatypeOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftConds\n",
      "http://www.w3.org/2004/02/skos/core#closeMatch\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfHasCheapSqlval\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfNiceSqlvalTmpl\n",
      "http://purl.org/goodrelations/v1#typeOfGood\n",
      "http://rdfs.org/ns/void#subset\n",
      "http://xmlns.com/foaf/0.1/name\n",
      "http://www.w3.org/ns/dcat#byteSize\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSubFormatForRefs\n",
      "http://purl.org/dc/terms/modified\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Step 1: Read entities from `u_types1_labels.csv`\n",
    "with open(\"Logs_properties.csv\", \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    entities = [row[0] for row in csv_reader]  # assuming the entity is in the first column\n",
    "\n",
    "# Step 2: Check if they exist in `1u-labels-u-kg-types1.txt`\n",
    "with open( \"10_Predicates_schema.csv\" , \"r\") as txt_file:\n",
    "    txt_content = txt_file.read().splitlines()\n",
    "\n",
    "entities_in_kg = [entity for entity in entities if entity in txt_content]\n",
    "entities_not_in_kg = [entity for entity in entities if entity not in txt_content]\n",
    "\n",
    "# Step 3: Write the existing entities to `EntitiesInQueriesExist_in_KG_schema.csv`\n",
    "with open(\"queries_valid_properties_KG_schema.csv\", \"w\", newline='') as csv_output:\n",
    "    csv_writer = csv.writer(csv_output)\n",
    "    for entity in entities_in_kg:\n",
    "        csv_writer.writerow([entity])\n",
    "\n",
    "# Step 4: Print the count and the entities that don't exist\n",
    "print(f\"Count of properties in queries that not exist in KG schema: {len(entities_not_in_kg)}\")\n",
    "\n",
    "print(\"\\n properties that don't exist in KG schema:\")\n",
    "for entity in entities_not_in_kg:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284b262-99c8-44d2-b5f6-02e9e6e93281",
   "metadata": {},
   "source": [
    "# Get total schema element of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b1e94e7-d58a-4613-8b61-c6280fcd7770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606 unique elements have been written to unique_elements.txt\n"
     ]
    }
   ],
   "source": [
    "# Set to hold the unique elements from both files\n",
    "unique_elements = set()\n",
    "\n",
    "# Read the property_types.txt file and add its contents to the set\n",
    "with open('queries_valid_properties_KG_schema.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        unique_elements.add(line.strip())  # strip() removes any leading/trailing whitespace including newline characters\n",
    "\n",
    "# Read the types1.csv file and add its contents to the set\n",
    "with open('queries_types_valid_KG_schema.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        unique_elements.add(line.strip())\n",
    "\n",
    "# Write the unique elements to a new file\n",
    "with open('Logs_total_schema_elements.txt', 'w') as file:\n",
    "    for element in unique_elements:\n",
    "        file.write(element + '\\n')\n",
    "\n",
    "print(f\"{len(unique_elements)} unique elements have been written to unique_elements.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935925b4-3c7d-4d22-b405-26d5dca9bc5d",
   "metadata": {},
   "source": [
    "# Get total schema element count of bio2rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ba517f93-25a4-4c7a-b186-6b327dd711ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1635 unique elements have been written to unique_elements.txt\n"
     ]
    }
   ],
   "source": [
    "# Set to hold the unique elements from both files\n",
    "unique_elements = set()\n",
    "\n",
    "# Read the property_types.txt file and add its contents to the set\n",
    "with open('10_sup_bio2rdf_spesific_Classes_schema.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        unique_elements.add(line.strip())  # strip() removes any leading/trailing whitespace including newline characters\n",
    "\n",
    "# Read the types1.csv file and add its contents to the set\n",
    "with open('10_Predicates_schema.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        unique_elements.add(line.strip())\n",
    "\n",
    "# Write the unique elements to a new file\n",
    "with open('10_bio2rdf_total_schema_elements.txt', 'w') as file:\n",
    "    for element in unique_elements:\n",
    "        file.write(element + '\\n')\n",
    "\n",
    "print(f\"{len(unique_elements)} unique elements have been written to unique_elements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550ec66d-da33-4425-ab77-352d491a61d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of entities in queries that exist in KG schema: 202\n",
      "\n",
      "Entities that don't exist in KG schema:\n",
      "http://rdfs.org/ns/void#subjectsTarget\n",
      "http://rdfs.org/ns/void#property\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfDatatypeOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvFormat\n",
      "http://purl.org/dc/terms/modified\n",
      "http://www.w3.org/2002/07/owl#priorVersion\n",
      "http://purl.org/dc/terms/hasPart\n",
      "http://bio2rdf.org/hgnc_vocabulary:Approved\n",
      "http://www.bigdata.com/rdf#/features/KB/Namespace\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfUriOfShortTmpl\n",
      "http://www.w3.org/ns/sparql-service-description#resultFormat\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfExistingShortOfUriTmpl\n",
      "http://purl.org/goodrelations/v1#legalName\n",
      "http://purl.org/goodrelations/v1#availableDeliveryMethods\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfColumnCount\n",
      "http://rdfs.org/ns/void#classes\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsBijection\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_4\n",
      "http://purl.org/ontology/wi/core#evidence\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfExistingShortOfTypedsqlvalTmpl\n",
      "http://www.w3.org/ns/sparql-service-description#defaultDataset\n",
      "http://www.w3.org/2000/01/rdf-schema#isDescribedUsing\n",
      "http://purl.org/pav/authoredBy\n",
      "http://purl.org/goodrelations/v1#BusinessEntity\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfLanguageTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfOkForAnySqlvalue\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsuriOfShortTmpl\n",
      "http://semanticscience.org/resource/SIO_000008\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvFText\n",
      "http://rdfs.org/ns/void#exampleResource\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsSubformatOfLongWhenRef\n",
      "http://www.openlinksw.com/schemas/virtrdf#isGcResistantType\n",
      "http://rdfs.org/ns/void#classPartition\n",
      "http://www.w3.org/ns/dcat#theme\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIslitOfShortTmpl\n",
      "http://www.w3.org/ns/sparql-service-description#supportedLanguage\n",
      "http://semanticscience.org/resource/SIO_000300\n",
      "http://purl.org/goodrelations/v1#typeOfGood\n",
      "http://rdfs.org/ns/void#objectsTarget\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsnumericOfShortTmpl\n",
      "http://purl.org/goodrelations/v1#eligibleCustomerTypes\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftTableName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmTableName\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_1\n",
      "http://www.w3.org/2004/02/skos/core#exactMatch\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfCmpFuncName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfValRange-rvrDatatype\n",
      "http://www.w3.org/2000/01/rdf-schema#domain\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftColumnName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftConds\n",
      "http://www.openlinksw.com/schemas/virtrdf#loadAs\n",
      "http://www.openlinksw.com/schemas/virtrdf#catName\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#TEL\n",
      "http://purl.org/goodrelations/v1#availableAtOrFrom\n",
      "http://xmlns.com/foaf/0.1/maker\n",
      "http://xmlns.com/foaf/0.1/logo\n",
      "http://purl.org/dc/terms/accrualPeriodicity\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmSubjectMap\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSuperFormats\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSqlvalTmpl\n",
      "http://www.w3.org/2004/02/skos/core#closeMatch\n",
      "http://www.w3.org/ns/sparql-service-description#url\n",
      "http://rdfs.org/ns/void#distinctObjects\n",
      "http://rdfs.org/ns/void#properties\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsSubformatOfLong\n",
      "http://xmlns.com/foaf/0.1/mbox\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsblankOfShortTmpl\n",
      "http://www.w3.org/2002/07/owl#versionInfo\n",
      "http://rdfs.org/ns/void#vocabulary\n",
      "http://purl.org/dc/terms/issued\n",
      "http://www.w3.org/ns/sparql-service-description#feature\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmPredicateMap\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfMapsOnlyNullToNull\n",
      "http://bio2rdf.org/drugbank_vocabulary:7cab3885cdbcb9df8c405e9c9ad10732\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfWrapDistinct\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfUriTmpl\n",
      "http://purl.org/pav/2.0/version\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftAlias\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_5\n",
      "http://semanticscience.org/resource/SIO_000628\n",
      "http://bio2rdf.org/hgnc_vocabulary:Status\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#EMAIL\n",
      "http://www.w3.org/2000/01/rdf-schema#type\n",
      "http://www.openlinksw.com/schemas/virtrdf#item\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvcColumnName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfValRange-rvrLanguage\n",
      "http://purl.org/pav/createdBy\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfLongTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvTableName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfValRange-rvrRestrictions\n",
      "http://purl.org/pav/version\n",
      "http://www.openlinksw.com/schemas/virtrdf#noInherit\n",
      "http://www.openlinksw.com/schemas/virtrdf#qsMatchingFlags\n",
      "http://purl.org/goodrelations/v1#eligibleRegions\n",
      "http://purl.org/dc/terms/language\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfExistingShortOfSqlvalTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSubFormatForRefs\n",
      "http://xmlns.com/foaf/0.1/name\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfTypemaxTmpl\n",
      "http://purl.uniprot.org/core/organism\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmGraphMap\n",
      "http://www.openlinksw.com/schemas/virtrdf#version\n",
      "http://www.openlinksw.com/schemas/DAV#ownerUser\n",
      "http://bio2rdf.org/atmo_vocabulary:Resource\n",
      "http://purl.org/goodrelations/v1#acceptedPaymentMethods\n",
      "http://semanticscience.org/resource/SIO_000062\n",
      "http://purl.org/pav/authoredOn\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSparqlEbvTmpl\n",
      "http://purl.org/pav/importedOn\n",
      "http://purl.org/goodrelations/v1#includesObject\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvATables\n",
      "http://purl.org/dc/terms/isVersionOf\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfDatatypeTmpl\n",
      "http://www.w3.org/ns/prov#wasDerivedFrom\n",
      "http://purl.org/goodrelations/v1#hasUnitOfMeasurement\n",
      "http://purl.org/goodrelations/v1#includes\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsSubformatOfLongWhenEqToSql\n",
      "http://purl.org/pav/createdWith\n",
      "http://xmlns.com/foaf/0.1/primaryTopic\n",
      "http://purl.org/pav/retrievedFrom\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfStrsqlvalOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvftXmlIndex\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#Street\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#Country\n",
      "http://rdfs.org/ns/void#triples\n",
      "http://purl.org/goodrelations/v1#hasBusinessFunction\n",
      "http://semanticscience.org/resource/SIO_000216\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfTypeminTmpl\n",
      "http://purl.org/goodrelations/v1#amountOfThisGood\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfCustomString1\n",
      "http://www.openlinksw.com/schemas/virtrdf#inheritFrom\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfNiceSqlvalTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfDtpOfNiceSqlval\n",
      "http://www.w3.org/2000/01/rdf-schema#range\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvcAlias\n",
      "http://purl.org/goodrelations/v1#validFrom\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSparqlEbvOfShortTmpl\n",
      "http://purl.org/dc/terms/extent\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvColumnsFormKey\n",
      "http://semanticscience.org/resource/SIO_000095\n",
      "http://www.w3.org/ns/dcat#downloadURL\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmObjectMap\n",
      "http://purl.org/dc/terms/conformsTo\n",
      "http://www.w3.org/ns/sparql-service-description#inputFormat\n",
      "http://semanticscience.org/resource/SIO_000205\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfExistingShortOfLongTmpl\n",
      "http://rdfs.org/ns/void#sparqlEndpoint\n",
      "http://semanticscience.org/resource/SIO_000253\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfHasCheapSqlval\n",
      "http://purl.org/dc/terms/references\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsStable\n",
      "http://www.w3.org/ns/sparql-service-description#defaultGraph\n",
      "http://www.w3.org/ns/dcat#byteSize\n",
      "http://rdfs.org/ns/void#propertyPartition\n",
      "http://semanticscience.org/resource/SIO_000001\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfLongTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmf01uriOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIidOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvGeo\n",
      "http://www.w3.org/2002/07/owl#complementOf\n",
      "http://xmlns.com/foaf/0.1/homepage\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfLongOfShortTmpl\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#ADR\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfLanguageOfShortTmpl\n",
      "http://rdfs.org/ns/void#linkPredicate\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfIsrefOfShortTmpl\n",
      "http://rdfs.org/ns/void#class\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfSqlvalTmpl\n",
      "http://purl.org/dc/terms/rightsHolder\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#City\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmf01blankOfShortTmpl\n",
      "http://purl.org/pav/lastUpdateOn\n",
      "http://purl.org/dc/terms/createdBy\n",
      "http://semanticscience.org/resource/SIO_000341\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_3\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvColumns\n",
      "http://semanticscience.org/resource/SIO_000772\n",
      "http://purl.org/goodrelations/v1#offers\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfBoolOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfSqlvalOfShortTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#dialect\n",
      "http://www.openlinksw.com/schemas/virtrdf#qsDefaultMap\n",
      "http://rdfs.org/ns/void#distinctSubjects\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvaTableName\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmMatchingFlags\n",
      "http://www.w3.org/2001/vcard-rdf/3.0#Pcode\n",
      "http://rdfs.org/ns/void#entities\n",
      "http://www.openlinksw.com/schemas/virtrdf#isSpecialPredicate\n",
      "http://www.w3.org/ns/sparql-service-description#endpoint\n",
      "http://rdfs.org/ns/void#uriSpace\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfBoolTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfShortOfTypedsqlvalTmpl\n",
      "http://www.openlinksw.com/schemas/virtrdf#qsUserMaps\n",
      "http://semanticscience.org/resource/SIO_010078\n",
      "http://purl.org/pav/createdOn\n",
      "http://purl.org/goodrelations/v1#hasPriceSpecification\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#_2\n",
      "http://rdfs.org/ns/void#subset\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmvaAlias\n",
      "http://purl.org/goodrelations/v1#validThrough\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Step 1: Read entities from `u_types1_labels.csv`\n",
    "with open(\"Logs_total_schema_elements.txt\" , \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    entities = [row[0] for row in csv_reader] # assuming the entity is in the first column\n",
    "\n",
    "# Step 2: Check if they exist in `1u-labels-u-kg-types1.txt`\n",
    "with open( \"10_bio2rdf_total_schema_elements.txt\"  , \"r\") as txt_file:\n",
    "    txt_content = txt_file.read().splitlines()\n",
    "\n",
    "entities_in_kg = [entity for entity in entities if entity in txt_content]\n",
    "entities_not_in_kg = [entity for entity in entities if entity not in txt_content]\n",
    "\n",
    "\n",
    "# Step 4: Print the count and the entities that don't exist\n",
    "print(f\"Count of entities in queries that exist in KG schema: {len(entities_not_in_kg)}\")\n",
    "print(\"\\nEntities that don't exist in KG schema:\")\n",
    "for entity in entities_not_in_kg:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c726546f-6c97-41ad-8acb-e84f0ce189ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of entities not in KG schema: 54\n",
      "\n",
      "Entities that don't exist in KG schema:\n",
      "http://bio2rdf.org/clinicaltrials_vocabulary:limitations-and-caveats\n",
      "http://bio2rdf.org/clinicaltrials_vocabulary:ci-upper-limit\n",
      "http://bio2rdf.org/clinicaltrials_vocabulary:lower-limit\n",
      "http://bio2rdf.org/obo_vocabulary:Entity\n",
      "http://bio2rdf.org/kegg_vocabulary:Reversible-Reaction\n",
      "http://bio2rdf.org/clinicaltrials_vocabulary:ci-upper-limit-na-comment\n",
      "http://bio2rdf.org/clinicaltrials_vocabulary:upper-limit\n",
      "http://bio2rdf.org/clinicaltrials_vocabulary:ci-lower-limit\n",
      "http://bio2rdf.org/kegg_vocabulary:Irreversible-Reaction\n",
      "http://bio2rdf.org/broad-lincrna_vocabulary:Resource\n",
      "http://bio2rdf.org/ed_vocabulary:Resource\n",
      "http://bio2rdf.org/ligandbox_vocabulary:Resource\n",
      "http://bio2rdf.org/ncbi-proteinid_vocabulary:Resource\n",
      "http://bio2rdf.org/solgenomics_vocabulary:Resource\n",
      "http://bio2rdf.org/luo_lincrna_vocabulary:Resource\n",
      "http://bio2rdf.org/signalp_vocabulary:Resource\n",
      "http://bio2rdf.org/utoronto_vocabulary:Resource\n",
      "http://bio2rdf.org/v_vocabulary:Resource\n",
      "http://bio2rdf.org/rnacentral_vocabulary:Resource\n",
      "http://bio2rdf.org/phi_vocabulary:Resource\n",
      "http://bio2rdf.org/jcggdb_vocabulary:Resource\n",
      "http://bio2rdf.org/phobius_vocabulary:Resource\n",
      "http://bio2rdf.org/imga_vocabulary:Resource\n",
      "http://bio2rdf.org/id_vocabulary:Resource\n",
      "http://bio2rdf.org/tmhmm_vocabulary:Resource\n",
      "http://bio2rdf.org/ucsc_genes_vocabulary:Resource\n",
      "http://bio2rdf.org/mwsh_vocabulary:Resource\n",
      "http://bio2rdf.org/epcc_vocabulary:Resource\n",
      "http://bio2rdf.org/goc_vocabulary:Resource\n",
      "http://bio2rdf.org/goeco_vocabulary:Resource\n",
      "http://ldf.fi/void-ext#subjectPartition\n",
      "http://www.ontologydesignpatterns.org/ont/dul/DUL.owl#expresses\n",
      "http://www.w3.org/ns/sparql-service-description#namedGraph\n",
      "http://vocabularies.bridgedb.org/ops#objectsDatatype\n",
      "http://vocabularies.bridgedb.org/ops#linksetJustification\n",
      "http://xmlns.com/foaf/0.1/primaryTopicOf\n",
      "http://www.w3.org/ns/dcat#landingPage\n",
      "http://www.openlinksw.com/schemas/virtrdf#qmfUriIdOffset\n",
      "http://www.openlinksw.com/schemas/virtrdf#dialect-exceptions\n",
      "http://www.w3.org/ns/sparql-service-description#name\n",
      "http://ldf.fi/void-ext#datatype\n",
      "http://www.w3.org/ns/prov#wasGeneratedBy\n",
      "http://xmlns.com/foaf/0.1/member\n",
      "http://www.openlinksw.com/schemas/virtrdf#bestRequestMethod\n",
      "http://rdfs.org/ns/void#datadump\n",
      "http://www.w3.org/ns/sparql-service-description#graph\n",
      "http://www.w3.org/ns/dcat#accessURL\n",
      "http://ldf.fi/void-ext#subject\n",
      "http://vocabularies.bridgedb.org/ops#subjectsDatatype\n",
      "http://ldf.fi/void-ext#datatypePartition\n",
      "http://purl.org/pav/previousVersion\n",
      "http://vocabularies.bridgedb.org/ops#assertionMethod\n",
      "http://schema.org/logo\n",
      "http://rdfs.org/ns/void#dataDump\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Read entities from `bio2rdf_total_schema_elements.txt`\n",
    "with open(\"bio2rdf_total_schema_elements.txt\", \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    entities = [row[0] for row in csv_reader]  # assuming the entity is in the first column\n",
    "\n",
    "# Check if they exist in `Logs_total_schema_elements.txt`\n",
    "with open(\"Logs_total_schema_elements.txt\", \"r\") as txt_file:\n",
    "    txt_content = txt_file.read().splitlines()\n",
    "\n",
    "# Determine which entities are not in the knowledge graph\n",
    "entities_not_in_kg = [entity for entity in entities if entity not in txt_content]\n",
    "\n",
    "# Step 4: Print the count and the entities that don't exist\n",
    "# We will sort the entities based on whether they contain '_vocabulary:Resource' or '_vocabulary'\n",
    "resource_entities = [entity for entity in entities_not_in_kg if '_vocabulary:Resource' in entity]\n",
    "vocabulary_entities = [entity for entity in entities_not_in_kg if '_vocabulary' in entity and entity not in resource_entities]\n",
    "other_entities = [entity for entity in entities_not_in_kg if entity not in resource_entities and entity not in vocabulary_entities]\n",
    "\n",
    "# Now we combine the lists, keeping the desired order\n",
    "sorted_entities_not_in_kg = vocabulary_entities + resource_entities + other_entities\n",
    "\n",
    "print(f\"Count of entities not in KG schema: {len(sorted_entities_not_in_kg)}\")\n",
    "print(\"\\nEntities that don't exist in KG schema:\")\n",
    "for entity in sorted_entities_not_in_kg:\n",
    "    print(entity)\n",
    "# 10_Subclass_all.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33779f3d-98bf-49fb-b675-01e149dd64e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of entities not in KG schema: 0\n",
      "\n",
      "Entities that don't exist in KG schema:\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Read entities from `10_Subclass_all.csv`\n",
    "entities = []\n",
    "with open(\"10_Subclass_all.csv\", \"r\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        if row:  # Check if row is not empty\n",
    "            entities.append(row[0])  # Assuming the entity is in the first column\n",
    "\n",
    "# Check if they exist in `10_bio2rdf_total_schema_elements.txt`\n",
    "with open(\"10_bio2rdf_total_schema_elements.txt\", \"r\") as txt_file:\n",
    "    txt_content = txt_file.read().splitlines()\n",
    "\n",
    "# Determine which entities are not in the knowledge graph\n",
    "entities_not_in_kg = [entity for entity in entities if entity not in txt_content]\n",
    "\n",
    "print(f\"Count of entities not in KG schema: {len(entities_not_in_kg)}\")\n",
    "print(\"\\nEntities that don't exist in KG schema:\")\n",
    "for entity in entities_not_in_kg:\n",
    "    print(entity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
